{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Nature Conservancy Fisheries Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fish_data as fd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fish_data:\n",
      "\n",
      "NAME\n",
      "    fish_data\n",
      "\n",
      "DESCRIPTION\n",
      "    fish_data module contains the helper functions for the model build of the\n",
      "    Nature Conservancy Fisheries Kaggle Competition.\n",
      "    \n",
      "    Dependencies:\n",
      "        * numpy as np\n",
      "        * os\n",
      "        * scipy.ndimage as ndimage\n",
      "        * scipy.misc as misc\n",
      "        * scipy.special as special\n",
      "        * matplotlib.pyplot as plt\n",
      "        * tensorflow as tf\n",
      "\n",
      "FUNCTIONS\n",
      "    count_nodes(y_in, x_in, conv_depths, conv_strides, pool_strides)\n",
      "        Calculates the number of total nodes present in the last layer of a\n",
      "        convolution plus max_pooling architecture.  Calculations assume that\n",
      "        convolution is 'SAME' padded, and pooling is 'VALID' padded.\n",
      "    \n",
      "    decode_image(image_read, size, num_channels=3, mutate=False, brightness_delta=None, contrast_limits=None, hue_delta=None, saturation_limits=None)\n",
      "        Converts a dequeued image read from filename to a single tensor array,\n",
      "        with modifications:\n",
      "            * resized to standard height and width supplied in size param\n",
      "            * normalized to mean near zero, min == -0.5, max ==  0.5\n",
      "            * distortions if mutate == True :\n",
      "                * random flip left right\n",
      "                * random flip up down\n",
      "                * # TODO : random crop and resize to standard size ???\n",
      "                * random brightness\n",
      "                * random contrast\n",
      "                * random hue\n",
      "                * random saturation\n",
      "        \n",
      "        providing distortion if mutate == True\n",
      "    \n",
      "    generate_balanced_filenames_epoch(min_each, shuffle=True)\n",
      "        Function to generate a list of filenames to be used for each training epoch\n",
      "        with a corresponding label array.  Most file names will be used  multiple  times\n",
      "        in order that each fish is drawn into a training batch an equivalent number of\n",
      "        times.\n",
      "    \n",
      "    generate_filenames_list()\n",
      "        Iterates through the 'data/train' folders of the working directory to\n",
      "        generate a list of filenames\n",
      "    \n",
      "    make_labels(filename_list, directory_string='train/', end_string='/img')\n",
      "        Receives a list of filenames and returns an ordered one-hot label\n",
      "        array by finding the fish species ID within the filename string.\n",
      "    \n",
      "    show_panel(image)\n",
      "\n",
      "FILE\n",
      "    /Users/ccthomps/Documents/Python Files/Kaggle Competitions/Nature Conservancy Fisheries/fish_data.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3777 filenames in the master set list\n"
     ]
    }
   ],
   "source": [
    "fish_filenames = fd.generate_filenames_list()\n",
    "print(\"There are {} filenames in the master set list\".format(len(fish_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_label_arr = fd.make_labels(fish_filenames, 'train/', '/img')\n",
    "fish_label_arr.shape\n",
    "fish_label_arr[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3702, 8), (75, 8)]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 75\n",
    "files_train, files_val, y_train, y_val = train_test_split(fish_filenames, fish_label_arr, test_size = valid_size)\n",
    "print([x.shape for x in [y_train, y_val]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging Graph and session calls with input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and Session Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "std_y = 300\n",
    "std_x = 500\n",
    "\n",
    "# General\n",
    "num_channels = 3\n",
    "num_labels = 8\n",
    "batch_size = 25\n",
    "stddev = 0.2\n",
    "\n",
    "# convolution\n",
    "kernel_sizes = [12, 3, 3, 3, 3, 3]\n",
    "conv_depths = [64, 128, 256, 512, 256, 128]\n",
    "conv_strides = [4, 1, 1, 1, 1, 1]\n",
    "\n",
    "pool_strides = [2, 2, 2, 2]\n",
    "\n",
    "final_depth = conv_depths[-1]\n",
    "\n",
    "#dropout\n",
    "kp = 0.75\n",
    "\n",
    "# fully connected\n",
    "fc1_depth = 256\n",
    "fc2_depth = 64\n",
    "\n",
    "#regularization\n",
    "beta = 1e-1 \n",
    "\n",
    "# Learning rate\n",
    "init_rate = 5e-3\n",
    "per_steps = 6000\n",
    "decay_rate = 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# epochs\n",
    "num_epochs = 5\n",
    "# path for tensorboard summary file to be written\n",
    "logs_path = os.getcwd()+'/TB_logs'\n",
    "valid_every = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i 'GRAPH.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "\n",
      "\n",
      "To view your tensorboard dashboard summary, run the following on the command line:\n",
      "tensorboard --logdir='/Users/ccthomps/Documents/Python Files/Kaggle Competitions/Nature Conservancy Fisheries/TB_logs'\n",
      "Train_Mean_Cross_entropy: 543133.6875\n",
      "     Valid_mean_cross_entropy: 338082.5625\n",
      "Train_Mean_Cross_entropy: 350593.875\n",
      "Train_Mean_Cross_entropy: 90598.796875\n",
      "Train_Mean_Cross_entropy: 32313.025390625\n",
      "Train_Mean_Cross_entropy: 53913.84375\n",
      "Train_Mean_Cross_entropy: 33691.0234375\n",
      "Train_Mean_Cross_entropy: 8119.458984375\n",
      "Train_Mean_Cross_entropy: 15893.068359375\n",
      "Train_Mean_Cross_entropy: 12028.447265625\n",
      "Train_Mean_Cross_entropy: 9965.16796875\n",
      "Train_Mean_Cross_entropy: 13634.5986328125\n",
      "     Valid_mean_cross_entropy: 201049.359375\n",
      "Train_Mean_Cross_entropy: 11598.40625\n",
      "Train_Mean_Cross_entropy: 6139.25927734375\n",
      "Train_Mean_Cross_entropy: 9656.85546875\n",
      "Train_Mean_Cross_entropy: 10683.2841796875\n",
      "Train_Mean_Cross_entropy: 7661.1298828125\n",
      "Train_Mean_Cross_entropy: 6785.009765625\n",
      "Train_Mean_Cross_entropy: 5206.216796875\n",
      "Train_Mean_Cross_entropy: 3505.3544921875\n",
      "Train_Mean_Cross_entropy: 4655.44384765625\n",
      "Train_Mean_Cross_entropy: 5173.10791015625\n",
      "     Valid_mean_cross_entropy: 26693.9609375\n",
      "Train_Mean_Cross_entropy: 2937.083984375\n",
      "Train_Mean_Cross_entropy: 5240.400390625\n",
      "Train_Mean_Cross_entropy: 4585.255859375\n",
      "Train_Mean_Cross_entropy: 6725.02001953125\n",
      "Train_Mean_Cross_entropy: 3797.402587890625\n",
      "Train_Mean_Cross_entropy: 4420.64111328125\n",
      "Train_Mean_Cross_entropy: 4049.077880859375\n",
      "Train_Mean_Cross_entropy: 4210.9267578125\n",
      "Train_Mean_Cross_entropy: 3119.814697265625\n",
      "Train_Mean_Cross_entropy: 2986.798828125\n",
      "     Valid_mean_cross_entropy: 1974.0108642578125\n",
      "Train_Mean_Cross_entropy: 2768.955078125\n",
      "Train_Mean_Cross_entropy: 2917.995361328125\n",
      "Train_Mean_Cross_entropy: 2303.19775390625\n",
      "Train_Mean_Cross_entropy: 3699.34033203125\n",
      "Train_Mean_Cross_entropy: 3139.12255859375\n",
      "Train_Mean_Cross_entropy: 2390.40869140625\n",
      "Train_Mean_Cross_entropy: 2600.931640625\n",
      "Train_Mean_Cross_entropy: 2316.053466796875\n",
      "Train_Mean_Cross_entropy: 3755.9296875\n",
      "Train_Mean_Cross_entropy: 2641.458740234375\n",
      "     Valid_mean_cross_entropy: 1611.8831787109375\n",
      "Train_Mean_Cross_entropy: 1802.09326171875\n",
      "Train_Mean_Cross_entropy: 2318.164306640625\n",
      "Train_Mean_Cross_entropy: 3773.412109375\n",
      "Train_Mean_Cross_entropy: 1544.3671875\n",
      "Train_Mean_Cross_entropy: 2726.619140625\n",
      "Train_Mean_Cross_entropy: 2106.43896484375\n",
      "Train_Mean_Cross_entropy: 2306.98974609375\n",
      "Train_Mean_Cross_entropy: 1670.160888671875\n",
      "Train_Mean_Cross_entropy: 2790.062255859375\n",
      "Train_Mean_Cross_entropy: 2219.982666015625\n",
      "     Valid_mean_cross_entropy: 1020.3026123046875\n",
      "Train_Mean_Cross_entropy: 2205.169677734375\n",
      "Train_Mean_Cross_entropy: 1841.0899658203125\n",
      "Train_Mean_Cross_entropy: 1573.87353515625\n",
      "Train_Mean_Cross_entropy: 2699.815673828125\n",
      "Train_Mean_Cross_entropy: 1758.300048828125\n",
      "Train_Mean_Cross_entropy: 1871.2716064453125\n",
      "Train_Mean_Cross_entropy: 1352.5791015625\n",
      "Train_Mean_Cross_entropy: 2047.828125\n",
      "Train_Mean_Cross_entropy: 1490.3280029296875\n",
      "Train_Mean_Cross_entropy: 2148.48681640625\n",
      "     Valid_mean_cross_entropy: 686.93798828125\n",
      "Train_Mean_Cross_entropy: 1362.959716796875\n",
      "Train_Mean_Cross_entropy: 1911.3714599609375\n",
      "Train_Mean_Cross_entropy: 1752.6605224609375\n",
      "Train_Mean_Cross_entropy: 1887.7733154296875\n",
      "Train_Mean_Cross_entropy: 1838.6090087890625\n",
      "Train_Mean_Cross_entropy: 1924.6534423828125\n",
      "Train_Mean_Cross_entropy: 1023.5662231445312\n",
      "Train_Mean_Cross_entropy: 1570.4754638671875\n",
      "Train_Mean_Cross_entropy: 1674.510498046875\n",
      "Train_Mean_Cross_entropy: 1514.9937744140625\n",
      "     Valid_mean_cross_entropy: 498.3935852050781\n",
      "Train_Mean_Cross_entropy: 1995.8731689453125\n",
      "Train_Mean_Cross_entropy: 1259.62158203125\n",
      "Train_Mean_Cross_entropy: 1724.92529296875\n",
      "Train_Mean_Cross_entropy: 1658.8748779296875\n",
      "Train_Mean_Cross_entropy: 1659.6585693359375\n",
      "Train_Mean_Cross_entropy: 1588.7618408203125\n",
      "Train_Mean_Cross_entropy: 1261.555419921875\n",
      "Train_Mean_Cross_entropy: 1604.3187255859375\n",
      "Train_Mean_Cross_entropy: 1832.6253662109375\n",
      "Train_Mean_Cross_entropy: 977.34423828125\n",
      "     Valid_mean_cross_entropy: 420.89569091796875\n",
      "Train_Mean_Cross_entropy: 1571.3731689453125\n",
      "Train_Mean_Cross_entropy: 1424.462646484375\n",
      "Train_Mean_Cross_entropy: 1315.0201416015625\n",
      "Train_Mean_Cross_entropy: 1655.378173828125\n",
      "Train_Mean_Cross_entropy: 1141.710205078125\n",
      "Train_Mean_Cross_entropy: 1382.2855224609375\n",
      "Train_Mean_Cross_entropy: 547.657958984375\n",
      "Train_Mean_Cross_entropy: 1340.7606201171875\n",
      "Train_Mean_Cross_entropy: 1163.6519775390625\n",
      "Train_Mean_Cross_entropy: 1487.4012451171875\n",
      "     Valid_mean_cross_entropy: 329.49493408203125\n",
      "Train_Mean_Cross_entropy: 1536.11376953125\n",
      "Train_Mean_Cross_entropy: 1306.71728515625\n",
      "Train_Mean_Cross_entropy: 1089.514404296875\n",
      "Train_Mean_Cross_entropy: 1341.8538818359375\n",
      "Train_Mean_Cross_entropy: 1793.8482666015625\n",
      "Train_Mean_Cross_entropy: 1433.188232421875\n",
      "Train_Mean_Cross_entropy: 1415.13134765625\n",
      "Train_Mean_Cross_entropy: 945.6810913085938\n",
      "Train_Mean_Cross_entropy: 1514.28515625\n"
     ]
    }
   ],
   "source": [
    "%run -i 'SESSION.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
