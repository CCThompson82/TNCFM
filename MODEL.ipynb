{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Nature Conservancy Fisheries Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fish_data as fd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fish_data:\n",
      "\n",
      "NAME\n",
      "    fish_data\n",
      "\n",
      "DESCRIPTION\n",
      "    fish_data module contains the helper functions for the model build of the\n",
      "    Nature Conservancy Fisheries Kaggle Competition.\n",
      "    \n",
      "    Dependencies:\n",
      "        * numpy as np\n",
      "        * os\n",
      "        * scipy.ndimage as ndimage\n",
      "        * scipy.misc as misc\n",
      "        * scipy.special as special\n",
      "        * matplotlib.pyplot as plt\n",
      "        * tensorflow as tf\n",
      "\n",
      "FUNCTIONS\n",
      "    count_nodes(x, y, kernel, stride, conv_depth, pad='SAME')\n",
      "        Calculates the number of total nodes present in the next layer of a\n",
      "        convolution OR max_pooling event.\n",
      "    \n",
      "    decode_image(image_name, size, num_channels=3, mean_channel_vals=[155.0, 155.0, 155.0], mutate=False, crop='random', crop_size=224)\n",
      "        Converts a dequeued image read from filename to a single tensor array,\n",
      "        with modifications:\n",
      "            * smallest dimension resized to standard height and width supplied in size param\n",
      "            * each channel centered to mean near zero.  Deviation is not normalized.\n",
      "            * if mutate == True :\n",
      "                * random flip left right\n",
      "                * random flip up down\n",
      "                * TODO : random colour adjustment\n",
      "                * random crop from standard size to crop size (e.g. 256x256 to 224x224)\n",
      "    \n",
      "    generate_balanced_filenames_epoch(min_each, shuffle=True)\n",
      "        Function to generate a list of filenames to be used for each training epoch\n",
      "        with a corresponding label array.  Most file names will be used  multiple  times\n",
      "        in order that each fish is drawn into a training batch an equivalent number of\n",
      "        times.\n",
      "    \n",
      "    generate_filenames_list(subdirectory='data/train/', subfolders=True)\n",
      "        Iterates through the default 'data/train' folders of the working directory to\n",
      "        generate a list of filenames\n",
      "    \n",
      "    make_labels(filename_list, directory_string='train/', end_string='/img')\n",
      "        Receives a list of filenames and returns an ordered one-hot label\n",
      "        array by finding the fish species ID within the filename string.\n",
      "    \n",
      "    show_panel(image)\n",
      "        Convenience function for showing an inline montage of the colour and merged channels\n",
      "\n",
      "FILE\n",
      "    /Users/ccthomps/Documents/Python Files/Kaggle Competitions/Nature Conservancy Fisheries/fish_data.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3777 filenames in the master set list\n",
      "There are 1000 filenames in the test set list\n"
     ]
    }
   ],
   "source": [
    "fish_filenames = fd.generate_filenames_list('data/train/', subfolders = True)\n",
    "print(\"There are {} filenames in the master set list\".format(len(fish_filenames)))\n",
    "test_filenames = fd.generate_filenames_list('data/test_stg1/', subfolders = False)\n",
    "print(\"There are {} filenames in the test set list\".format(len(test_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Dictionary of image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid set filename dimensions downloaded correctly: True\n",
      "Training/Valid set filename dimensions downloaded correctly: True\n"
     ]
    }
   ],
   "source": [
    "with open('dimensions_dict.json') as f:\n",
    "    dim_dict = json.load(f)\n",
    "    \n",
    "print(\"Training/Valid set filename dimensions downloaded correctly: {}\".format(\n",
    "        dim_dict.get(fish_filenames[0]) == [720, 1280, 3]))\n",
    "print(\"Training/Valid set filename dimensions downloaded correctly: {}\".format(\n",
    "        dim_dict.get(test_filenames[0]) == [720, 1280, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the labels for the master set list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One label per row entry: True\n"
     ]
    }
   ],
   "source": [
    "fish_label_arr = fd.make_labels(fish_filenames, 'train/', '/img')\n",
    "fish_label_arr.shape\n",
    "print(\"One label per row entry: {}\".format(all(np.sum(fish_label_arr, 1) == 1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and split the master set list into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 300\n",
      "Training set size: 3477\n"
     ]
    }
   ],
   "source": [
    "valid_size = 300\n",
    "files_train, files_val, y_train, y_val = train_test_split(fish_filenames, fish_label_arr, test_size = valid_size)\n",
    "print(\"Validation set size: {}\".format(y_val.shape[0]))\n",
    "print(\"Training set size: {}\".format(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a files_train list that represents each class of fish equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Need to refactor generate_balanced_filenames to work from this list, not from scratch.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Need to refactor generate_balanced_filenames to work from this list, not from scratch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dims_list = []\n",
    "for f in files_train :\n",
    "    train_dims_list.append(dim_dict.get(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and Session Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for each entry: 224x224x3 = 150528\n",
      "Dimensions after first convolution step (with max pool): 27x27x96 = 69984\n",
      "Dimensions after second convolution step (with max pool): 13x13x256 = 43264\n",
      "Dimensions after third convolution step: 13x13x384 = 64896\n",
      "Dimensions after fourth convolution step: 13x13x384 = 64896\n",
      "Dimensions after fifth convolution step (with max pool): 6x6x256 = 9216\n",
      "Dimensions after first connected layer: 4096\n",
      "Dimensions after second connected layer: 2048\n",
      "Final dimensions for classification: 8\n"
     ]
    }
   ],
   "source": [
    "%run -i 'PARAMETERS.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version_ID = 'v2.0.0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i 'GRAPH.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "\n",
      "\n",
      "To view your tensorboard dashboard summary, run the following on the command line:\n",
      "tensorboard --logdir='/Users/ccthomps/Documents/Python Files/Kaggle Competitions/Nature Conservancy Fisheries/TB_logs/v2.0.0.2'\n",
      "\n",
      "Batch number: 1\n",
      "     Training_mean_cross_entropy: 2.0615084171295166\n",
      "     Valid_mean_cross_entropy: 2.016716718673706\n",
      "[[ 0.14332998 -0.11647757  0.04149004  0.1926288  -0.06375211 -0.08433591\n",
      "   0.13266997  0.06963497]\n",
      " [ 0.14355561 -0.11767981  0.04131452  0.19433841 -0.06332918 -0.08434913\n",
      "   0.13230242  0.06896063]]\n",
      "Batch number: 5\n",
      "     Training_mean_cross_entropy: 1.7985576391220093\n",
      "     Valid_mean_cross_entropy: 1.8652681112289429\n",
      "[[ 0.22698489 -0.12943108  0.01835114  0.16600189 -0.07970841 -0.09153789\n",
      "   0.10572385  0.10160749]\n",
      " [ 0.22733074 -0.13064615  0.01812139  0.16750154 -0.07934919 -0.09158377\n",
      "   0.10531699  0.10091592]]\n",
      "Batch number: 9\n",
      "     Training_mean_cross_entropy: 1.7907931804656982\n",
      "     Valid_mean_cross_entropy: 1.6612229347229004\n",
      "[[ 0.9037928  -0.27471426 -0.21442431 -0.09237532 -0.04264342 -0.19981547\n",
      "  -0.07309303  0.29508057]\n",
      " [ 0.90511817 -0.27598989 -0.21445833 -0.09087936 -0.04251987 -0.19938429\n",
      "  -0.07389932  0.29503378]]\n",
      "Batch number: 11\n",
      "     Training_mean_cross_entropy: 1.7384310960769653\n",
      "     Valid_mean_cross_entropy: 1.6191086769104004\n",
      "[[ 1.73219168 -0.5160954  -0.49129042 -0.45604604  0.08759364 -0.30316177\n",
      "  -0.33192655  0.55422169]\n",
      " [ 1.73478448 -0.51788521 -0.49126038 -0.4552092   0.08762065 -0.30256787\n",
      "  -0.33323738  0.55491447]]\n",
      "Batch number: 13\n",
      "     Training_mean_cross_entropy: 1.5925614833831787\n",
      "     Valid_mean_cross_entropy: 1.603993535041809\n",
      "[[ 1.97185016 -0.60640776 -0.62284911 -0.70436805  0.17267592 -0.26293162\n",
      "  -0.48213363  0.77968782]\n",
      " [ 1.97488296 -0.60833275 -0.6227898  -0.70403218  0.1731728  -0.26202035\n",
      "  -0.48368901  0.78080261]]\n",
      "Batch number: 17\n",
      "     Training_mean_cross_entropy: 1.599314570426941\n",
      "     Valid_mean_cross_entropy: 1.5872764587402344\n",
      "[[ 1.95477474 -0.61524606 -0.68550986 -0.79636723  0.24326438 -0.20803811\n",
      "  -0.55217928  0.88113368]\n",
      " [ 1.9579823  -0.61702019 -0.68555039 -0.79633564  0.24365507 -0.20712119\n",
      "  -0.55369443  0.88232613]]\n",
      "Batch number: 21\n",
      "     Training_mean_cross_entropy: 1.489353060722351\n",
      "     Valid_mean_cross_entropy: 1.5970505475997925\n",
      "[[ 1.74884164 -0.55289167 -0.84585106 -1.02258205  0.46980008  0.00224318\n",
      "  -0.64117414  1.05027092]\n",
      " [ 1.75197828 -0.55468887 -0.84622765 -1.02289927  0.47015977  0.00275532\n",
      "  -0.64304924  1.05154228]]\n",
      "Batch number: 25\n",
      "     Training_mean_cross_entropy: 1.8178718090057373\n",
      "     Valid_mean_cross_entropy: 1.6131376028060913\n",
      "[[ 1.56619179 -0.47957441 -0.92934245 -1.14278769  0.71445757  0.11053202\n",
      "  -0.68175125  1.04344022]\n",
      " [ 1.56924117 -0.48114726 -0.92961174 -1.1431694   0.71486735  0.11076747\n",
      "  -0.68350488  1.0448873 ]]\n",
      "Batch number: 29\n",
      "     Training_mean_cross_entropy: 1.6561769247055054\n",
      "     Valid_mean_cross_entropy: 1.622350811958313\n",
      "[[ 1.52324498 -0.39916903 -0.93780768 -1.20975518  0.75221598  0.11198974\n",
      "  -0.68131787  1.0748626 ]\n",
      " [ 1.52615905 -0.40075177 -0.93841404 -1.2101047   0.75291878  0.11215492\n",
      "  -0.6830793   1.07623374]]\n",
      "Batch number: 31\n",
      "     Training_mean_cross_entropy: 1.6231086254119873\n",
      "     Valid_mean_cross_entropy: 1.6311964988708496\n",
      "[[ 1.54839945 -0.29392919 -0.96677589 -1.26931465  0.89836103  0.07289965\n",
      "  -0.66412038  0.92544031]\n",
      " [ 1.55077875 -0.29548165 -0.96764612 -1.2698524   0.89918095  0.07306563\n",
      "  -0.66545188  0.92637521]]\n",
      "Batch number: 33\n",
      "     Training_mean_cross_entropy: 1.3571830987930298\n",
      "     Valid_mean_cross_entropy: 1.6340219974517822\n",
      "[[ 1.68411326 -0.27002573 -0.99925548 -1.30206823  0.93411982  0.07862084\n",
      "  -0.62180358  0.76419687]\n",
      " [ 1.68650508 -0.2714707  -1.00021553 -1.30285192  0.9349013   0.07875714\n",
      "  -0.62301511  0.76485986]]\n",
      "Batch number: 37\n",
      "     Training_mean_cross_entropy: 1.783534049987793\n",
      "     Valid_mean_cross_entropy: 1.6335158348083496\n",
      "[[ 1.78734219 -0.27412236 -1.03161168 -1.32559323  0.92957938  0.11490767\n",
      "  -0.60949779  0.69759703]\n",
      " [ 1.78982449 -0.27555791 -1.03244889 -1.32644641  0.93028146  0.1150297\n",
      "  -0.61056596  0.69823325]]\n",
      "Batch number: 41\n",
      "     Training_mean_cross_entropy: 1.2739473581314087\n",
      "     Valid_mean_cross_entropy: 1.6246198415756226\n",
      "[[ 1.93082154 -0.3108696  -1.05372894 -1.3632381   0.87264776  0.1523677\n",
      "  -0.55346119  0.64697069]\n",
      " [ 1.93318272 -0.31236508 -1.05471623 -1.36400867  0.87301815  0.15248147\n",
      "  -0.55454326  0.64743727]]\n",
      "Batch number: 45\n",
      "     Training_mean_cross_entropy: 1.6554884910583496\n",
      "     Valid_mean_cross_entropy: 1.6082203388214111\n",
      "[[ 1.90129602 -0.3550185  -1.09618342 -1.37482786  0.79457551  0.24967401\n",
      "  -0.51266873  0.7529543 ]\n",
      " [ 1.90353787 -0.35651937 -1.09683371 -1.37589955  0.79480755  0.24989393\n",
      "  -0.51382011  0.7534225 ]]\n",
      "Batch number: 49\n",
      "     Training_mean_cross_entropy: 1.316527247428894\n",
      "     Valid_mean_cross_entropy: 1.6102712154388428\n",
      "[[ 1.71605551 -0.33078834 -1.13429964 -1.39770937  0.83011323  0.24385403\n",
      "  -0.48783043  0.94096082]\n",
      " [ 1.71807146 -0.3322219  -1.13484597 -1.39874387  0.83006954  0.24411015\n",
      "  -0.48871377  0.94152385]]\n",
      "Batch number: 51\n",
      "     Training_mean_cross_entropy: 1.6538532972335815\n",
      "     Valid_mean_cross_entropy: 1.6073359251022339\n",
      "[[ 1.64253259 -0.29192263 -1.11281478 -1.39912736  0.80586922  0.15601262\n",
      "  -0.45927742  1.05816185]\n",
      " [ 1.64398766 -0.29358864 -1.11323345 -1.39993191  0.80567098  0.15587518\n",
      "  -0.46014702  1.05882156]]\n"
     ]
    }
   ],
   "source": [
    "%run -i 'SESSION.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Notes during run \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(W.shape[3]) :\n",
    "    print(i)\n",
    "    plt.imshow(W[:,:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
